



==================================================================================================
			### Kubernetes cluster - Multinode on CentOS Stream 9  ###
==================================================================================================

DNS Server 
172.16.24.81	primary-dns.csisrlab.wilp.bits-pilani.ac.in
172.16.24.82	secondary-dns.csisrlab.wilp.bits-pilani.ac.in


- System Requirements for Kubernetes cluster 1.29.2
--------------------------------------------------------------------------------------------------------------------------------------------------------------
TYPE			HOSTNAME									IP ADDRESS			NIC				VCPU	RAM		OS-HDD		OS	
---------------------------------------------------------------------------------------------------------------------------------------------------------------
controller		k8master01.csisrlab.wilp.bits-pilani.ac.in 	172.16.24.83		2 {eno1,eno2}	8		16		1 TB		CentOS Stream 9
controller		k8master02.csisrlab.wilp.bits-pilani.ac.in	172.16.24.84		2 {eno1,eno2}	8		16		1 TB		CentOS Stream 9
controller		k8master03.csisrlab.wilp.bits-pilani.ac.in	172.16.24.85		2 {eno1,eno2}	8		16		1 TB		CentOS Stream 9
VIP				k8vip.csisrlab.wilp.bits-pilani.ac.in		172.16.24.86		- Here Am Using VIP for LoadBalance HAProxy+Keepalived 
---------------------------------------------------------------------------------------------------------------------------------------------------------------


1] - Set hostname for all nodes and add /etc/host file update

# systemctl set-hostname <hostname>

cat >>/etc/hosts <<EOF
172.16.24.83	k8master01.csisrlab.wilp.bits-pilani.ac.in
172.16.24.84	k8master02.csisrlab.wilp.bits-pilani.ac.in
172.16.24.85	k8master03.csisrlab.wilp.bits-pilani.ac.in
172.16.24.86	k8vip.csisrlab.wilp.bits-pilani.ac.in
EOF
-------------------------------------------------------------------------------------------


2] Set-NTP and restart service in all nodes

# timedatectl set-timezone Asia/Kolkata
# systemctl restart chronyd && systemctl enable chronyd && systemctl status chronyd 
-------------------------------------------------------------------------------------------

3]  Disable Firewalls and SELinux in all nodes

# systemctl stop firewalld && systemctl disable firewalld

			- If Use firewalls
			# systemctl start firewalld && systemctl enable firewalld

					# firewall-cmd --permanent --add-port=6443/tcp && firewall-cmd --reload
					# firewall-cmd --permanent --add-port=2379-2380/tcp && firewall-cmd --reload
					# firewall-cmd --permanent --add-port=10250/tcp && firewall-cmd --reload
					# firewall-cmd --permanent --add-port=10251/tcp && firewall-cmd --reload
					# firewall-cmd --permanent --add-port=10252/tcp && firewall-cmd --reload
					# firewall-cmd --permanent --add-port=179/tcp && firewall-cmd --reload
					# firewall-cmd --permanent --add-port=4789/udp && firewall-cmd --reload
--------------------------------------------------------------------------------------------

4] - SELinux off and Turn-Off Swap (all nodes)

# setenforce 0
# sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config


# swapoff -a
# sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
--------------------------------------------------------------------------------------------


5] - Load two required modules and add configuration to make them loadable at boot time.

# modprobe overlay
# modprobe br_netfilter

cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF


cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

# sysctl --system
--------------------------------------------------------------------------------------------

6] Password less SSH for all nodes
# ssh-keygen
# ssh-copy-id root@172.16.24.83

[root@k8master01 ~]# for i in 83 84 85;do ssh root@172.16.24.$i hostname;done
k8master01.csisrlab.wilp.bits-pilani.ac.in
k8master02.csisrlab.wilp.bits-pilani.ac.in
k8master03.csisrlab.wilp.bits-pilani.ac.in

--------------------------------------------------------------------------------------------


7] Install HAProxy and Keepalived and configure in all nodes

# yum install haproxy keepalived -y

	- Note - Configure Keepalived on master01 first, create check_apiserver.sh script following content


cat <<EOF | sudo tee /etc/keepalived/check_apiserver.sh
#!/bin/sh
APISERVER_VIP=172.16.24.86
APISERVER_DEST_PORT=6443
errorExit() {
echo "*** $*" 1>&2
exit 1
}
curl --silent --max-time 2 --insecure https://localhost:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit "Error GET https://localhost:${APISERVER_DEST_PORT}/"
if ip addr | grep -q ${APISERVER_VIP}; then
curl --silent --max-time 2 --insecure https://${APISERVER_VIP}:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit "Error GET https://${APISERVER_VIP}:${APISERVER_DEST_PORT}/"
fi
EOF


# chmod +x /etc/keepalived/check_apiserver.sh
--------------------------------------------------------------------------------------------


8] -	Note: Only two parameters of this file need to be changed for master02 & master03 nodes.
		State will become SLAVE for master02 and master03, priority will be 254 and 253 respectively.
		
- Configure Keepalived 

# mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.org


cat >> /etc/keepalived/keepalived.conf << EOF
! Configuration File for keepalived
global_defs {
   router_id LB_01
   }
vrrp_instance VI_1 {
    state MASTER
    interface eno1
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        172.16.24.86
    }
}
EOF
---------------------------------------------------------------------

9] - Configure HAProxy 

# mv /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.org


cat >> /etc/haproxy/haproxy.cfg << EOF
#---------------------------------------------------------------------
# apiserver frontend which proxys to the masters
#---------------------------------------------------------------------
frontend apiserver
     bind *:8443
     mode tcp
     option tcplog
     default_backend apiserver
#---------------------------------------------------------------------
# round robin balancing between the various backends
#---------------------------------------------------------------------
backend apiserver
    option httpchk GET /healthz
    http-check expect status 200
    mode tcp
    option ssl-hello-chk
    balance     roundrobin
       server  k8master01.csisrlab.wilp.bits-pilani.ac.in 172.16.24.90:6443 check
       server  k8master02.csisrlab.wilp.bits-pilani.ac.in 172.16.24.91:6443 check
       server  k8master03.csisrlab.wilp.bits-pilani.ac.in 172.16.24.92:6443 check
EOF
------------------------------------------------------------------------------------------------------------


10] - Now copy theses Three files (check_apiserver.sh , keepalived.conf and haproxy.cfg) from master01 to master02 & master03
	- Master02 and Master03 take backup or change haproxy.cfg keepalived.conf to haproxy.cfg.org keepalived.conf.org

# mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.org
# mv /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg.org


- Now copy master01 to master02, master03 

[root@master01 ~]# for f in 172.16.24.84 172.16.24.85; do scp /etc/keepalived/check_apiserver.sh /etc/keepalived/keepalived.conf root@$f:/etc/keepalived; scp /etc/haproxy/haproxy.cfg root@$f:/etc/haproxy; done
check_apiserver.sh                                                                                                      100%  347   483.0KB/s   00:00
keepalived.conf                                                                                                         100%  317   557.3KB/s   00:00
haproxy.cfg                                                                                                             100%  874     1.3MB/s   00:00
check_apiserver.sh                                                                                                      100%  347   559.9KB/s   00:00
keepalived.conf                                                                                                         100%  317   654.8KB/s   00:00
haproxy.cfg                                                                                                             100%  874     1.4MB/s   00:00


	
- Finally start and enable keepalived and haproxy service on all three master nodes

# systemctl enable keepalived && systemctl restart keepalived
# systemctl enable haproxy && systemctl restart haproxy
------------------------------------------------------------------------------------------------------------
	
	
11] - After coping 3 files edit the keepalived.conf in master02 and master03 below changes

- Master02

# vim /etc/keepalived/keepalived.conf

! Configuration File for keepalived
global_defs {
   router_id LB_02
   }
vrrp_instance VI_2 {
    state BACKUP
    interface eno1
    virtual_router_id 52
    priority 101
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        172.16.24.86
    }
}



- Master03

# vim /etc/keepalived/keepalived.conf

! Configuration File for keepalived
global_defs {
   router_id LB_03
   }
vrrp_instance VI_3 {
    state BACKUP
    interface eno1
    virtual_router_id 53
    priority 102
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        172.16.24.86
    }
}


	- Finally start and enable keepalived and haproxy service on master02, master03

# systemctl enable keepalived && systemctl restart keepalived
# systemctl enable haproxy && systemctl restart haproxy


	
	
- Veryfy - Once these services are started successfully, verify whether VIP (virtual IP) is enabled on master01 node because we have marked master01 as MASTER node in keepalived configuration file.

[root@master01 ~]# ip a s
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:50:56:ab:bd:83 brd ff:ff:ff:ff:ff:ff
    altname enp2s1
    inet 172.16.24.90/23 brd 172.16.25.255 scope global noprefixroute ens33
       valid_lft forever preferred_lft forever
    inet 172.16.24.93/32 scope global ens33
       valid_lft forever preferred_lft forever
--------------------------------------------------------------------------------------------



12] -  Install containerd in all nodes 	- Add the official Docker repository.

# dnf config-manager --add-repo=https://download.docker.com/linux/centos/docker-ce.repo
# dnf update -y
# dnf install -y containerd.io

- Create a configuration file for containerd and set it to default, in all nodes 
# mkdir -p /etc/containerd
# containerd config default | sudo tee /etc/containerd/config.toml
--------------------------------------------------------------------------------------------


13] - Change false to true in all nodes 
	- Set cgroupdriver to systemd - kubelet requires the cgroupdriver to be set to systemd. 
	- Find the following section: [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options] 125 line false to true


# vim /etc/containerd/config.toml 

SystemdCgroup = true

	- Restart containerd - To apply the changes made in the last step, restart containerd.

# systemctl restart containerd && systemctl enable containerd && systemctl status containerd
--------------------------------------------------------------------------------------------

	- Verify that containerd is running using this command:
[root@master01 ~]# ps -ef | grep containerd
root       50457       1  0 20:52 ?        00:00:00 /usr/bin/containerd
root       50500    6005  0 20:53 pts/0    00:00:00 grep --color=auto containerd
- -------------------------------------------------------------------------------------------


14] - Install below mentioned packages (All nodes):

# yum install -y wget vim-enhanced git curl


15] - Add the Kubernetes repository

cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF


16] -  Install Modules
	- Update your machines and then install all Kubernetes modules.

# yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
# systemctl enable --now kubelet
- -------------------------------------------------------------------------------------------


17] - Deploy the Cluster

[root@k8master01 ~]# kubeadm init --control-plane-endpoint "k8vip.csisrlab.wilp.bits-pilani.ac.in:8443" --upload-certs --pod-network-cidr=10.244.0.0/16
[init] Using Kubernetes version: v1.29.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0401 01:22:17.205690    9067 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8master01.csisrlab.wilp.bits-pilani.ac.in k8vip.csisrlab.wilp.bits-pilani.ac.in kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.24.83]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8master01.csisrlab.wilp.bits-pilani.ac.in localhost] and IPs [172.16.24.83 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8master01.csisrlab.wilp.bits-pilani.ac.in localhost] and IPs [172.16.24.83 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
W0401 01:22:39.371980    9067 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "admin.conf" kubeconfig file
W0401 01:22:39.799308    9067 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "super-admin.conf" kubeconfig file
W0401 01:22:40.130239    9067 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "kubelet.conf" kubeconfig file
W0401 01:22:40.416257    9067 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
W0401 01:22:40.506839    9067 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 16.514944 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
cd775f72141f7ac7200f598c879134e2c99fe508685f3ce7f3d0bb7ebb16e251
[mark-control-plane] Marking the node k8master01.csisrlab.wilp.bits-pilani.ac.in as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8master01.csisrlab.wilp.bits-pilani.ac.in as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: gj2y2y.y2dv5bep0hbnxwm4
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
W0401 01:23:04.957583    9067 endpoint.go:57] [endpoint] WARNING: port specified in controlPlaneEndpoint overrides bindPort in the controlplane address
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join k8vip.csisrlab.wilp.bits-pilani.ac.in:8443 --token gj2y2y.y2dv5bep0hbnxwm4 \
        --discovery-token-ca-cert-hash sha256:c268b5d4dbdb90fce3dac95dd845bb8401cfa5027b8feab73968c23e26a54665 \
        --control-plane --certificate-key cd775f72141f7ac7200f598c879134e2c99fe508685f3ce7f3d0bb7ebb16e251

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join k8vip.csisrlab.wilp.bits-pilani.ac.in:8443 --token gj2y2y.y2dv5bep0hbnxwm4 \
        --discovery-token-ca-cert-hash sha256:c268b5d4dbdb90fce3dac95dd845bb8401cfa5027b8feab73968c23e26a54665



========================================================================================================================

18] Configure

[root@master01 ~]# mkdir -p $HOME/.kube
[root@master01 ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@master01 ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config



19] - Deploy the pod Network.

	- Link : https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises
	- Note: Here we can see our master server is not in a ready state because we havenâ€™t applied the pod network to the cluster. Once we apply the pod network all containers will come up.

# curl https://raw.githubusercontent.com/projectcalico/calico/v3.27.2/manifests/calico.yaml -O
# kubectl apply -f calico.yaml

- wait for few min and Now we need to verify the Nodes status and pods status.

[root@k8master01 ~]# kubectl get pods -A
NAMESPACE     NAME                                                                 READY   STATUS    RESTARTS        AGE
kube-system   calico-kube-controllers-68cdf756d9-pdmwb                             1/1     Running   0               10m
kube-system   calico-node-4nrhs                                                    1/1     Running   0               86s
kube-system   calico-node-mr4lw                                                    1/1     Running   0               5m47s
kube-system   calico-node-sm8sj                                                    1/1     Running   0               10m
kube-system   coredns-76f75df574-4x9dr                                             1/1     Running   0               19m
kube-system   coredns-76f75df574-thp7z                                             1/1     Running   0               19m
kube-system   etcd-k8master01.csisrlab.wilp.bits-pilani.ac.in                      1/1     Running   0               19m
kube-system   etcd-k8master02.csisrlab.wilp.bits-pilani.ac.in                      1/1     Running   0               5m39s
kube-system   etcd-k8master03.csisrlab.wilp.bits-pilani.ac.in                      1/1     Running   0               76s
kube-system   kube-apiserver-k8master01.csisrlab.wilp.bits-pilani.ac.in            1/1     Running   0               19m
kube-system   kube-apiserver-k8master02.csisrlab.wilp.bits-pilani.ac.in            1/1     Running   1 (4m33s ago)   5m40s
kube-system   kube-apiserver-k8master03.csisrlab.wilp.bits-pilani.ac.in            1/1     Running   0               76s
kube-system   kube-controller-manager-k8master01.csisrlab.wilp.bits-pilani.ac.in   1/1     Running   0               19m
kube-system   kube-controller-manager-k8master02.csisrlab.wilp.bits-pilani.ac.in   1/1     Running   0               5m38s
kube-system   kube-controller-manager-k8master03.csisrlab.wilp.bits-pilani.ac.in   1/1     Running   0               77s
kube-system   kube-proxy-nwnjj                                                     1/1     Running   0               86s
kube-system   kube-proxy-wn9hp                                                     1/1     Running   0               5m47s
kube-system   kube-proxy-xprh7                                                     1/1     Running   0               19m
kube-system   kube-scheduler-k8master01.csisrlab.wilp.bits-pilani.ac.in            1/1     Running   0               19m
kube-system   kube-scheduler-k8master02.csisrlab.wilp.bits-pilani.ac.in            1/1     Running   0               5m39s
kube-system   kube-scheduler-k8master03.csisrlab.wilp.bits-pilani.ac.in            1/1     Running   0               78s



[root@k8master01 ~]# kubectl get nodes
NAME       STATUS   ROLES           AGE     VERSION
k8master01.csisrlab.wilp.bits-pilani.ac.in   Ready    control-plane   19m    v1.29.3
---------------------------------------------------------------------------------------------------------------------------------

- Add master02 and master03

[root@k8master01 ~]# kubeadm join vip.csisrlab.wilp.bits-pilani.ac.in:8443 --token zthzz7.7jiqglv92p135b1m \
        --discovery-token-ca-cert-hash sha256:f2fc60eebc7643105632f6fd45a2fe09abbb6d46db208ddbd0c018762850f2aa \
        --control-plane --certificate-key 1a2b73ee6a771fe9213d987e96ad2ddd855958c14037eb93cc65d42c66c4e12e

- Add master03
[root@k8master01 ~]# kubeadm join vip.csisrlab.wilp.bits-pilani.ac.in:8443 --token zthzz7.7jiqglv92p135b1m \
        --discovery-token-ca-cert-hash sha256:f2fc60eebc7643105632f6fd45a2fe09abbb6d46db208ddbd0c018762850f2aa \
        --control-plane --certificate-key 1a2b73ee6a771fe9213d987e96ad2ddd855958c14037eb93cc65d42c66c4e12e

---------------------------------------------------------------------------------------------------------------------------------

20] - Once the master02, master03 added check the status

[root@k8master01 ~]# kubectl get nodes
NAME                                         STATUS   ROLES           AGE    VERSION
k8master01.csisrlab.wilp.bits-pilani.ac.in   Ready    control-plane   19m    v1.29.3
k8master02.csisrlab.wilp.bits-pilani.ac.in   Ready    control-plane   6m7s   v1.29.3
k8master03.csisrlab.wilp.bits-pilani.ac.in   Ready    control-plane   105s   v1.29.3




[root@k8master01 ~]# kubectl get nodes -o wide
NAME                                         STATUS   ROLES           AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE          KERNEL-VERSION          CONTAINER-RUNTIME
k8master01.csisrlab.wilp.bits-pilani.ac.in   Ready    control-plane   20m     v1.29.3   172.16.24.83   <none>        CentOS Stream 9   5.14.0-432.el9.x86_64   containerd://1.6.28
k8master02.csisrlab.wilp.bits-pilani.ac.in   Ready    control-plane   7m10s   v1.29.3   172.16.24.84   <none>        CentOS Stream 9   5.14.0-432.el9.x86_64   containerd://1.6.28
k8master03.csisrlab.wilp.bits-pilani.ac.in   Ready    control-plane   2m48s   v1.29.3   172.16.24.85   <none>        CentOS Stream 9   5.14.0-432.el9.x86_64   containerd://1.6.28


==============================================================================================================================


















